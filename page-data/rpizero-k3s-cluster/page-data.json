{
    "componentChunkName": "component---src-templates-blog-post-js",
    "path": "/rpizero-k3s-cluster/",
    "result": {"data":{"site":{"siteMetadata":{"title":"andreveiga.dev"}},"markdownRemark":{"id":"b07049ca-de4e-58eb-8f01-8458154e150d","excerpt":"Introduction This week I wanted to challenge myself and try to create a Kubernetes cluster on 3 of my Raspberry Pi Zero 2 W‚Äôs. These Pi‚Äôs are somewhat‚Ä¶","html":"<h1>Introduction</h1>\n<p>This week I wanted to challenge myself and try to create a Kubernetes cluster on 3 of my Raspberry Pi Zero 2 W‚Äôs. These Pi‚Äôs are somewhat underpowered for the task, especially in regards to their RAM, and I hoped to learn a trick or two on setting up High Availability on such a constrained environment.</p>\n<p>If you‚Äôve read my previous posts, you‚Äôll know that I‚Äôm using <a href=\"https://k3s.io\">k3s</a>, from Rancher Labs. It‚Äôs light-weight, full-featured, and highly configurable. The perfect fit for this use case.</p>\n<h1>Solution</h1>\n<p>Highly available cluster will always need 3 Kubernetes nodes, at the very least, as per the <a href=\"https://raft.github.io\">Raft Algorithm</a> used by etcd, the Kubernetes Control Plane database. This is, of course, assuming that application workloads are schedulable to the master nodes. If that‚Äôs not the case, as best practices dictate, then you need an Highly Available Control Plane (3 or more nodes) plus a number of worker nodes, depending on your own workload.</p>\n<p>In this exercise, for simplicity purposes, I‚Äôm using three Raspberry Pi Zero W‚Äôs that will run both the Control Plane as well as the application workloads. At least that was the plan‚Ä¶</p>\n<h2>Attempt #1: Running the standard k3s installation - Failed</h2>\n<p>My first approach was to ignore k3s‚Äô minimum hardware requirements and simply run the installer with no further deployment customizations. This proved to be impossible, as the k3s daemon didn‚Äôt even start due to insufficient memory on the Raspberry Pi. Nonetheless, this is how I went about it:</p>\n<p>The first node is pretty simple, just run the regular k3s install script:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> -sfL https://get.k3s.io <span class=\"token operator\">|</span> <span class=\"token function\">sh</span> -s - server --cluster-init</code></pre></div>\n<p>The next two Kubernetes nodes are setup by running:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> -sfL https://get.k3s.io <span class=\"token operator\">|</span> <span class=\"token function\">sh</span> -s - server --server https://<span class=\"token operator\">&lt;</span>ip or <span class=\"token function\">hostname</span> of server<span class=\"token operator\"><span class=\"token file-descriptor important\">1</span>></span>:6443</code></pre></div>\n<p>This should give you an HA cluster where the master nodes are schedulable. Not possible with Pi Zero 2‚Äôs, apparently, but more than possible with Pi 4‚Äôs (or even Pi 3‚Äôs).</p>\n<h2>Attempt #2: Externalizing the Control Plane DB</h2>\n<p>By default, k3s will run an embedded etcd as the Control Plane DB. It‚Äôs well documented that this solution may have <a href=\"https://rancher.com/docs/k3s/latest/en/installation/ha-embedded/\">‚Äúperformance issues on slower disks such as Raspberry Pis running with SD cards‚Äù</a>, so I though running an external DB to act as the Kubernetes Control Plane would maybe easy the load on the Pi Zero‚Äôs‚Ä¶ Luckily, this is supported out-of-the-box by k3s.</p>\n<p>First of all, you have to choose between using PostgreSQL, MySQL, MariaDB and (external) etcd. I went with PostgreSQL as I‚Äôm more familiar with it.</p>\n<p>Since this needs to run externally (and the point is to remove the extra load from the Pi‚Äôs), I‚Äôm running PostreSQL in Docker in my own Mac. This is the docker-compose file I‚Äôve used:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">version</span><span class=\"token punctuation\">:</span> <span class=\"token string\">\"3.0\"</span>\n<span class=\"token key atrule\">services</span><span class=\"token punctuation\">:</span>\n  <span class=\"token key atrule\">postgres</span><span class=\"token punctuation\">:</span>\n    <span class=\"token key atrule\">image</span><span class=\"token punctuation\">:</span> postgres\n    <span class=\"token key atrule\">restart</span><span class=\"token punctuation\">:</span> always\n    <span class=\"token key atrule\">volumes</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> pg<span class=\"token punctuation\">:</span>/var/lib/postgresql/data/pgdata\n    <span class=\"token key atrule\">ports</span><span class=\"token punctuation\">:</span>\n      <span class=\"token punctuation\">-</span> <span class=\"token string\">\"5450:5432\"</span>\n    <span class=\"token key atrule\">environment</span><span class=\"token punctuation\">:</span>\n      <span class=\"token key atrule\">POSTGRES_PASSWORD</span><span class=\"token punctuation\">:</span> postgres\n      <span class=\"token key atrule\">PGDATA</span><span class=\"token punctuation\">:</span> /var/lib/postgresql/data/pgdata\n\n<span class=\"token key atrule\">volumes</span><span class=\"token punctuation\">:</span>\n  pg<span class=\"token punctuation\">:</span></code></pre></div>\n<p>After spinning up the PostgreSQL container, you‚Äôll need to create a DB called <code class=\"language-text\">kubernetes</code>.</p>\n<p>We now need to pass the DB connection string to the k3s server setup commands.</p>\n<p>In this setup, since etcd isn‚Äôt really running in the Kubernetes cluster, we only really need two nodes to achieve High Availability (even though we‚Äôd have to deal with PostgreSQL high availability as well‚Ä¶ but that‚Äôs a different topic).</p>\n<p>The following command can be run in all nodes to create a cluster:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> -sfL https://get.k3s.io <span class=\"token operator\">|</span> <span class=\"token function\">sh</span> -s - server --token<span class=\"token operator\">=</span><span class=\"token operator\">&lt;</span>SECRET<span class=\"token operator\">></span> --datastore-endpoint<span class=\"token operator\">=</span><span class=\"token string\">\"postgres://postgres:postgres@&lt;PostgreSQL Server IP Address>:5450/kubernetes?sslmode=disable\"</span></code></pre></div>\n<p>In the end, this proved not to be enough. The rest of the Control Plane components are still too heavy for the Pi Zero‚Äôs‚Ä¶</p>\n<h2>Attempt #3: Splitting the control plane components over the different Pi‚Äôs - Also failed</h2>\n<p>k3s provides a few useful flags to disable the Kubernetes Control Plane components. Plus, remember when I mentioned that k3s is a full-featured Kubernetes distro? What that means is that k3s comes bundled with a number of useful components that are commonly needed to run a production cluster: Load Balancer, Ingress Controller, Local Storage Provider and a Metrics Server.</p>\n<p>While these are indeed needed in most situations where k3s is meant to be run, that‚Äôs not really the case for my exercise where I‚Äôm learning about k3s sizing and cluster stability. Thus, my next though was to:</p>\n<ol>\n<li>Disable all unnecessary components</li>\n<li>Run the different Control Plane components on different Pi‚Äôs</li>\n</ol>\n<p>This has a number of disadvantages, namely the complete lack of High Availability with just 3 servers (what if the Pi running etcd goes down? Or the Pi running the API Server?). This is how you can run k3s, disabling, for example, the Scheduler and Traefik (the Ingress Controller):</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> -sfL https://get.k3s.io <span class=\"token operator\">|</span> <span class=\"token function\">sh</span> -s - server --cluster-init --disable-scheduler --disable traefik</code></pre></div>\n<p>Even in this scenario it was still too much for the poor Pi Zero‚Äôs. You can learn more about k3s feature flags <a href=\"https://rancher.com/docs/k3s/latest/en/installation/install-options/server-config/\">here</a> and about etcd only nodes <a href=\"https://rancher.com/docs/k3s/latest/en/installation/disable-flags/\">here</a></p>\n<h2>Attempt #4: Running the control plane on an external VM - Great Success üöÄ</h2>\n<p>So‚Ä¶ that was it. I had to accept that running the Control Plane on the Pi Zero‚Äôs was a bit too much, for now‚Ä¶ Which shouldn‚Äôt be a surprise, given <a href=\"https://rancher.com/docs/k3s/latest/en/installation/installation-requirements/resource-profiling/\">k3s‚Äô minimum hardware requirements</a>.</p>\n<p>Nonetheless, I wanted to follow thorough with this. A highly available app deployment will still be running even if the Control Plane goes down! This means that we can run an entire master node externally and use the Pi‚Äôs as worker nodes, to deploy highly available app workloads.</p>\n<p>I‚Äôve then created a VM on my Mac where I‚Äôve installed a k3s master node:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> -fL https://get.k3s.io <span class=\"token operator\">|</span> <span class=\"token function\">sh</span> -s - server</code></pre></div>\n<p>And run the agent node installation on the Raspberry Pi‚Äôs as follows:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\"><span class=\"token function\">curl</span> -fL https://get.k3s.io <span class=\"token operator\">|</span> <span class=\"token function\">sh</span> -s - agent</code></pre></div>\n<p>There is, of course, a bit more to this. k3s can consume installation configuration from a <code class=\"language-text\">config.yaml</code> file which is preemptively placed at <code class=\"language-text\">/etc/rancher/k3s/</code>.</p>\n<p>The master node config file looks like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">node-name: <span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inventory_hostname <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\ncluster-init: <span class=\"token boolean\">true</span>\nnode-taint: <span class=\"token string\">\"CriticalAddonsOnly=true:NoExecute\"</span></code></pre></div>\n<p>and the agent node config file looks like this:</p>\n<div class=\"gatsby-highlight\" data-language=\"bash\"><pre class=\"language-bash\"><code class=\"language-bash\">node-name: <span class=\"token punctuation\">{</span><span class=\"token punctuation\">{</span> inventory_hostname <span class=\"token punctuation\">}</span><span class=\"token punctuation\">}</span>\ntoken: <span class=\"token string\">\"{{ hostvars['vm']['token'].stdout }}\"</span>\nserver: <span class=\"token string\">\"https://{{ hostvars['vm']['ansible_host'] }}:6443\"</span></code></pre></div>\n<p>Also, a few boot command line options need to be set on all Raspberry Pi‚Äôs. The <code class=\"language-text\">/boot/cmdline.txt</code> should look like this (notice the <code class=\"language-text\">cgroup_memory=1 cgroup_enable=memory</code> options at the end of the line):</p>\n<div class=\"gatsby-highlight\" data-language=\"text\"><pre class=\"language-text\"><code class=\"language-text\">console=serial-1,115200 console=tty1 root=PARTUUID=01decd83-02 rootfstype=ext4 fsck.repair=yes rootwait cgroup_memory=1 cgroup_enable=memory</code></pre></div>\n<p>By the way, did you notice the strange <code class=\"language-text\">{{ ... }}</code> syntax in some of the code sample above? This leads me to the next section‚Ä¶</p>\n<h1>Automation</h1>\n<p>As always, my true documentation is on GitHub, delivery as a set of Ansible Playbooks. You can <a href=\"https://github.com/aveiga/rpizero-k3s-cluster\">take a look at it, clone it and/or fork it here</a>.</p>\n<p>This has been thoroughly tested running the Playbooks from my M1 Macbook Air, and will result on a VM running the Kubernetes cluster master node (unschedulable for app workloads) and the three Raspberry Pi‚Äôs running the cluster‚Äôs worker nodes.</p>\n<p>Don‚Äôt forget to adapt the <a href=\"https://github.com/aveiga/rpizero-k3s-cluster/blob/main/hosts.yaml\">hosts.yaml</a> file to your own environment, as well as recreating the Ansible <a href=\"https://github.com/aveiga/rpizero-k3s-cluster/blob/main/group_vars/all/vault\">Vault</a> defining your own variables:</p>\n<div class=\"gatsby-highlight\" data-language=\"yaml\"><pre class=\"language-yaml\"><code class=\"language-yaml\"><span class=\"token key atrule\">vaulted_become_password</span><span class=\"token punctuation\">:</span> TBD <span class=\"token comment\"># The password used to run commands with elevated privileges</span>\n<span class=\"token key atrule\">vaulted_ssh_user</span><span class=\"token punctuation\">:</span> TBD <span class=\"token comment\"># The user used to SSH into the Pi's</span></code></pre></div>","frontmatter":{"title":"Setting up a Kubernetes cluster using Raspberry Pi Zero 2 W's","date":"May 17, 2022","description":"Deploying k3s to create a Kubernetes cluster running on top of 3 Raspberry Pi Zero 2 W's and using external Kubernetes control plane"}},"previous":{"fields":{"slug":"/automation/"},"frontmatter":{"title":"Documentation, Automation and Ansible"}},"next":null},"pageContext":{"id":"b07049ca-de4e-58eb-8f01-8458154e150d","previousPostId":"d0e3ab55-f664-5547-b1a7-730b37e3cece","nextPostId":null}},
    "staticQueryHashes": ["2841359383","3257411868"]}